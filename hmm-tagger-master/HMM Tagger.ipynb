{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Project: Part of Speech Tagging with Hidden Markov Models \n",
    "---\n",
    "### Introduction\n",
    "\n",
    "Part of speech tagging is the process of determining the syntactic category of a word from the words in its surrounding context. It is often used to help disambiguate natural language phrases because it can be done quickly with high accuracy. Tagging can be used for many NLP tasks like determining correct pronunciation during speech synthesis (for example, _dis_-count as a noun vs dis-_count_ as a verb), for information retrieval, and for word sense disambiguation.\n",
    "\n",
    "In this notebook, you'll use the [Pomegranate](http://pomegranate.readthedocs.io/) library to build a hidden Markov model for part of speech tagging using a \"universal\" tagset. Hidden Markov models have been able to achieve [>96% tag accuracy with larger tagsets on realistic text corpora](http://www.coli.uni-saarland.de/~thorsten/publications/Brants-ANLP00.pdf). Hidden Markov models have also been used for speech recognition and speech generation, machine translation, gene recognition for bioinformatics, and human gesture recognition for computer vision, and more. \n",
    "\n",
    "![](_post-hmm.png)\n",
    "\n",
    "The notebook already contains some code to get you started. You only need to add some new functionality in the areas indicated to complete the project; you will not need to modify the included code beyond what is requested. Sections that begin with **'IMPLEMENTATION'** in the header indicate that you must provide code in the block that follows. Instructions will be provided for each section, and the specifics of the implementation are marked in the code block with a 'TODO' statement. Please be sure to read the instructions carefully!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "**Note:** Once you have completed all of the code implementations, you need to finalize your work by exporting the iPython Notebook as an HTML document. Before exporting the notebook to html, all of the code cells need to have been run so that reviewers can see the final implementation and output. You must then **export the notebook** by running the last cell in the notebook, or by using the menu above and navigating to **File -> Download as -> HTML (.html)** Your submissions should include both the `html` and `ipynb` files.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "**Note:** Code and Markdown cells can be executed using the `Shift + Enter` keyboard shortcut. Markdown cells can be edited by double-clicking the cell to enter edit mode.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The Road Ahead\n",
    "You must complete Steps 1-3 below to pass the project. The section on Step 4 includes references & resources you can use to further explore HMM taggers.\n",
    "\n",
    "- [Step 1](#Step-1:-Read-and-preprocess-the-dataset): Review the provided interface to load and access the text corpus\n",
    "- [Step 2](#Step-2:-Build-a-Most-Frequent-Class-tagger): Build a Most Frequent Class tagger to use as a baseline\n",
    "- [Step 3](#Step-3:-Build-an-HMM-tagger): Build an HMM Part of Speech tagger and compare to the MFC baseline\n",
    "- [Step 4](#Step-4:-[Optional]-Improving-model-performance): (Optional) Improve the HMM tagger"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-warning\">\n",
    "**Note:** Make sure you have selected a **Python 3** kernel in Workspaces or the hmm-tagger conda environment if you are running the Jupyter server on your own machine.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Jupyter \"magic methods\" -- only need to be run once per kernel restart\n",
    "%load_ext autoreload\n",
    "%aimport helpers, tests\n",
    "%autoreload 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import python modules -- this cell needs to be run again if you make changes to any of the files\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "from IPython.core.display import HTML\n",
    "from itertools import chain\n",
    "from collections import Counter, defaultdict\n",
    "from helpers import show_model, Dataset\n",
    "from pomegranate import State, HiddenMarkovModel, DiscreteDistribution"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1: Read and preprocess the dataset\n",
    "---\n",
    "We'll start by reading in a text corpus and splitting it into a training and testing dataset. The data set is a copy of the [Brown corpus](https://en.wikipedia.org/wiki/Brown_Corpus) (originally from the [NLTK](https://www.nltk.org/) library) that has already been pre-processed to only include the [universal tagset](https://arxiv.org/pdf/1104.2086.pdf). You should expect to get slightly higher accuracy using this simplified tagset than the same model would achieve on a larger tagset like the full [Penn treebank tagset](https://www.ling.upenn.edu/courses/Fall_2003/ling001/penn_treebank_pos.html), but the process you'll follow would be the same.\n",
    "\n",
    "The `Dataset` class provided in helpers.py will read and parse the corpus. You can generate your own datasets compatible with the reader by writing them to the following format. The dataset is stored in plaintext as a collection of words and corresponding tags. Each sentence starts with a unique identifier on the first line, followed by one tab-separated word/tag pair on each following line. Sentences are separated by a single blank line.\n",
    "\n",
    "Example from the Brown corpus. \n",
    "```\n",
    "b100-38532\n",
    "Perhaps\tADV\n",
    "it\tPRON\n",
    "was\tVERB\n",
    "right\tADJ\n",
    ";\t.\n",
    ";\t.\n",
    "\n",
    "b100-35577\n",
    "...\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 57340 sentences in the corpus.\n",
      "There are 45872 sentences in the training set.\n",
      "There are 11468 sentences in the testing set.\n"
     ]
    }
   ],
   "source": [
    "data = Dataset(\"tags-universal.txt\", \"brown-universal.txt\", train_test_split=0.8)\n",
    "\n",
    "print(\"There are {} sentences in the corpus.\".format(len(data)))\n",
    "print(\"There are {} sentences in the training set.\".format(len(data.training_set)))\n",
    "print(\"There are {} sentences in the testing set.\".format(len(data.testing_set)))\n",
    "\n",
    "assert len(data) == len(data.training_set) + len(data.testing_set), \\\n",
    "       \"The number of sentences in the training set + testing set should sum to the number of sentences in the corpus\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The Dataset Interface\n",
    "\n",
    "You can access (mostly) immutable references to the dataset through a simple interface provided through the `Dataset` class, which represents an iterable collection of sentences along with easy access to partitions of the data for training & testing. Review the reference below, then run and review the next few cells to make sure you understand the interface before moving on to the next step.\n",
    "\n",
    "```\n",
    "Dataset-only Attributes:\n",
    "    training_set - reference to a Subset object containing the samples for training\n",
    "    testing_set - reference to a Subset object containing the samples for testing\n",
    "\n",
    "Dataset & Subset Attributes:\n",
    "    sentences - a dictionary with an entry {sentence_key: Sentence()} for each sentence in the corpus\n",
    "    keys - an immutable ordered (not sorted) collection of the sentence_keys for the corpus\n",
    "    vocab - an immutable collection of the unique words in the corpus\n",
    "    tagset - an immutable collection of the unique tags in the corpus\n",
    "    X - returns an array of words grouped by sentences ((w11, w12, w13, ...), (w21, w22, w23, ...), ...)\n",
    "    Y - returns an array of tags grouped by sentences ((t11, t12, t13, ...), (t21, t22, t23, ...), ...)\n",
    "    N - returns the number of distinct samples (individual words or tags) in the dataset\n",
    "\n",
    "Methods:\n",
    "    stream() - returns an flat iterable over all (word, tag) pairs across all sentences in the corpus\n",
    "    __iter__() - returns an iterable over the data as (sentence_key, Sentence()) pairs\n",
    "    __len__() - returns the nubmer of sentences in the dataset\n",
    "```\n",
    "\n",
    "For example, consider a Subset, `subset`, of the sentences `{\"s0\": Sentence((\"See\", \"Spot\", \"run\"), (\"VERB\", \"NOUN\", \"VERB\")), \"s1\": Sentence((\"Spot\", \"ran\"), (\"NOUN\", \"VERB\"))}`. The subset will have these attributes:\n",
    "\n",
    "```\n",
    "subset.keys == {\"s1\", \"s0\"}  # unordered\n",
    "subset.vocab == {\"See\", \"run\", \"ran\", \"Spot\"}  # unordered\n",
    "subset.tagset == {\"VERB\", \"NOUN\"}  # unordered\n",
    "subset.X == ((\"Spot\", \"ran\"), (\"See\", \"Spot\", \"run\"))  # order matches .keys\n",
    "subset.Y == ((\"NOUN\", \"VERB\"), (\"VERB\", \"NOUN\", \"VERB\"))  # order matches .keys\n",
    "subset.N == 7  # there are a total of seven observations over all sentences\n",
    "len(subset) == 2  # because there are two sentences\n",
    "```\n",
    "\n",
    "<div class=\"alert alert-block alert-info\">\n",
    "**Note:** The `Dataset` class is _convenient_, but it is **not** efficient. It is not suitable for huge datasets because it stores multiple redundant copies of the same data.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Sentences\n",
    "\n",
    "`Dataset.sentences` is a dictionary of all sentences in the training corpus, each keyed to a unique sentence identifier. Each `Sentence` is itself an object with two attributes: a tuple of the words in the sentence named `words` and a tuple of the tag corresponding to each word named `tags`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sentence: b100-38532\n",
      "words:\n",
      "\t('Perhaps', 'it', 'was', 'right', ';', ';')\n",
      "tags:\n",
      "\t('ADV', 'PRON', 'VERB', 'ADJ', '.', '.')\n"
     ]
    }
   ],
   "source": [
    "key = 'b100-38532'\n",
    "print(\"Sentence: {}\".format(key))\n",
    "print(\"words:\\n\\t{!s}\".format(data.sentences[key].words))\n",
    "print(\"tags:\\n\\t{!s}\".format(data.sentences[key].tags))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "**Note:** The underlying iterable sequence is **unordered** over the sentences in the corpus; it is not guaranteed to return the sentences in a consistent order between calls. Use `Dataset.stream()`, `Dataset.keys`, `Dataset.X`, or `Dataset.Y` attributes if you need ordered access to the data.\n",
    "</div>\n",
    "\n",
    "#### Counting Unique Elements\n",
    "\n",
    "You can access the list of unique words (the dataset vocabulary) via `Dataset.vocab` and the unique list of tags via `Dataset.tagset`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are a total of 1161192 samples of 56057 unique words in the corpus.\n",
      "There are 928458 samples of 50536 unique words in the training set.\n",
      "There are 232734 samples of 25112 unique words in the testing set.\n",
      "There are 5521 words in the test set that are missing in the training set.\n"
     ]
    }
   ],
   "source": [
    "print(\"There are a total of {} samples of {} unique words in the corpus.\"\n",
    "      .format(data.N, len(data.vocab)))\n",
    "print(\"There are {} samples of {} unique words in the training set.\"\n",
    "      .format(data.training_set.N, len(data.training_set.vocab)))\n",
    "print(\"There are {} samples of {} unique words in the testing set.\"\n",
    "      .format(data.testing_set.N, len(data.testing_set.vocab)))\n",
    "print(\"There are {} words in the test set that are missing in the training set.\"\n",
    "      .format(len(data.testing_set.vocab - data.training_set.vocab)))\n",
    "\n",
    "assert data.N == data.training_set.N + data.testing_set.N, \\\n",
    "       \"The number of training + test samples should sum to the total number of samples\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Accessing word and tag Sequences\n",
    "The `Dataset.X` and `Dataset.Y` attributes provide access to ordered collections of matching word and tag sequences for each sentence in the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sentence 1: ('Mr.', 'Podger', 'had', 'thanked', 'him', 'gravely', ',', 'and', 'now', 'he', 'made', 'use', 'of', 'the', 'advice', '.')\n",
      "\n",
      "Labels 1: ('NOUN', 'NOUN', 'VERB', 'VERB', 'PRON', 'ADV', '.', 'CONJ', 'ADV', 'PRON', 'VERB', 'NOUN', 'ADP', 'DET', 'NOUN', '.')\n",
      "\n",
      "Sentence 2: ('But', 'there', 'seemed', 'to', 'be', 'some', 'difference', 'of', 'opinion', 'as', 'to', 'how', 'far', 'the', 'board', 'should', 'go', ',', 'and', 'whose', 'advice', 'it', 'should', 'follow', '.')\n",
      "\n",
      "Labels 2: ('CONJ', 'PRT', 'VERB', 'PRT', 'VERB', 'DET', 'NOUN', 'ADP', 'NOUN', 'ADP', 'ADP', 'ADV', 'ADV', 'DET', 'NOUN', 'VERB', 'VERB', '.', 'CONJ', 'DET', 'NOUN', 'PRON', 'VERB', 'VERB', '.')\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# accessing words with Dataset.X and tags with Dataset.Y \n",
    "for i in range(2):    \n",
    "    print(\"Sentence {}:\".format(i + 1), data.X[i])\n",
    "    print()\n",
    "    print(\"Labels {}:\".format(i + 1), data.Y[i])\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Accessing (word, tag) Samples\n",
    "The `Dataset.stream()` method returns an iterator that chains together every pair of (word, tag) entries across all sentences in the entire corpus."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Stream (word, tag) pairs:\n",
      "\n",
      "\t Mr. 0\n",
      "\t Podger 1\n",
      "\t had 2\n",
      "\t thanked 3\n",
      "\t him 4\n",
      "\t gravely 5\n",
      "\t , 6\n"
     ]
    }
   ],
   "source": [
    "# use Dataset.stream() (word, tag) samples for the entire corpus\n",
    "print(\"\\nStream (word, tag) pairs:\\n\")\n",
    "for i, pair in enumerate(data.stream()):\n",
    "    print(\"\\t\", pair[0],i)\n",
    "    if i > 5: break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "For both our baseline tagger and the HMM model we'll build, we need to estimate the frequency of tags & words from the frequency counts of observations in the training corpus. In the next several cells you will complete functions to compute the counts of several sets of counts. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2: Build a Most Frequent Class tagger\n",
    "---\n",
    "\n",
    "Perhaps the simplest tagger (and a good baseline for tagger performance) is to simply choose the tag most frequently assigned to each word. This \"most frequent class\" tagger inspects each observed word in the sequence and assigns it the label that was most often assigned to that word in the corpus."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### IMPLEMENTATION: Pair Counts\n",
    "\n",
    "Complete the function below that computes the joint frequency counts for two input sequences."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div class=\"alert alert-block alert-success\">Your emission counts look good!</div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def pair_counts(sequences_A, sequences_B):\n",
    "    \"\"\"Return a dictionary keyed to each unique value in the first sequence list\n",
    "    that counts the number of occurrences of the corresponding value from the\n",
    "    second sequences list.\n",
    "    \n",
    "    For example, if sequences_A is tags and sequences_B is the corresponding\n",
    "    words, then if 1244 sequences contain the word \"time\" tagged as a NOUN, then\n",
    "    you should return a dictionary such that pair_counts[NOUN][time] == 1244\n",
    "    \"\"\"\n",
    "    res = dict()\n",
    "    for tag in np.unique(sequences_A):\n",
    "        res[tag] =Counter(np.array(sequences_B)[np.array(sequences_A)==tag])\n",
    "        \n",
    "    return res\n",
    "\n",
    "\n",
    "# Calculate C(t_i, w_i)\n",
    "tagss = [pair[1] for pair in data.stream()]\n",
    "wordss = [pair[0]for pair in data.stream()]\n",
    "emission_counts = pair_counts(tagss,wordss)\n",
    "\n",
    "assert len(emission_counts) == 12, \\\n",
    "       \"Uh oh. There should be 12 tags in your dictionary.\"\n",
    "assert max(emission_counts[\"NOUN\"], key=emission_counts[\"NOUN\"].get) == 'time', \\\n",
    "       \"Hmmm...'time' is expected to be the most common NOUN.\"\n",
    "HTML('<div class=\"alert alert-block alert-success\">Your emission counts look good!</div>')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### IMPLEMENTATION: Most Frequent Class Tagger\n",
    "\n",
    "Use the `pair_counts()` function and the training dataset to find the most frequent class label for each word in the training data, and populate the `mfc_table` below. The table keys should be words, and the values should be the appropriate tag string.\n",
    "\n",
    "The `MFCTagger` class is provided to mock the interface of Pomegranite HMM models so that they can be used interchangeably."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div class=\"alert alert-block alert-success\">Your MFC tagger has all the correct words!</div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create a lookup table mfc_table where mfc_table[word] contains the tag label most frequently assigned to that word\n",
    "from collections import namedtuple\n",
    "from operator import itemgetter\n",
    "FakeState = namedtuple(\"FakeState\", \"name\")\n",
    "\n",
    "class MFCTagger:\n",
    "    # NOTE: You should not need to modify this class or any of its methods\n",
    "    missing = FakeState(name=\"<MISSING>\")\n",
    "    \n",
    "    def __init__(self, table):\n",
    "        self.table = defaultdict(lambda: MFCTagger.missing)\n",
    "        self.table.update({word: FakeState(name=tag) for word, tag in table.items()})\n",
    "        \n",
    "    def viterbi(self, seq):\n",
    "        \"\"\"This method simplifies predictions by matching the Pomegranate viterbi() interface\"\"\"\n",
    "        return 0., list(enumerate([\"<start>\"] + [self.table[w] for w in seq] + [\"<end>\"]))\n",
    "\n",
    "\n",
    "# TODO: calculate the frequency of each tag being assigned to each word (hint: similar, but not\n",
    "# the same as the emission probabilities) and use it to fill the mfc_table\n",
    "\n",
    "word_counts = pair_counts(tagss,wordss)\n",
    "ress = []\n",
    "mfc_table=dict()\n",
    "ress = []\n",
    "for tagg,wordd in word_counts.items():\n",
    "    ress.append(list(itemgetter(*tuple(data.training_set.vocab))(word_counts[tagg])))\n",
    "mfc_table=dict(zip(data.training_set.vocab,list(np.array(list(word_counts.keys()))[np.argmax(np.array(ress),axis=0)])))\n",
    "mfc_model = MFCTagger(mfc_table) # Create a Most Frequent Class tagger instance\n",
    "\n",
    "assert len(mfc_table) == len(data.training_set.vocab), \"\"\n",
    "assert all(k in data.training_set.vocab for k in mfc_table.keys()), \"\"\n",
    "assert sum(int(k not in mfc_table) for k in data.testing_set.vocab) == 5521, \"\"\n",
    "HTML('<div class=\"alert alert-block alert-success\">Your MFC tagger has all the correct words!</div>')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Making Predictions with a Model\n",
    "The helper functions provided below interface with Pomegranate network models & the mocked MFCTagger to take advantage of the [missing value](http://pomegranate.readthedocs.io/en/latest/nan.html) functionality in Pomegranate through a simple sequence decoding function. Run these functions, then run the next cell to see some of the predictions made by the MFC tagger."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def replace_unknown(sequence):\n",
    "    \"\"\"Return a copy of the input sequence where each unknown word is replaced\n",
    "    by the literal string value 'nan'. Pomegranate will ignore these values\n",
    "    during computation.\n",
    "    \"\"\"\n",
    "    return [w if w in data.training_set.vocab else 'nan' for w in sequence]\n",
    "\n",
    "def simplify_decoding(X, model):\n",
    "    \"\"\"X should be a 1-D sequence of observations for the model to predict\"\"\"\n",
    "    _, state_path = model.viterbi(replace_unknown(X))\n",
    "    return [state[1].name for state in state_path[1:-1]]  # do not show the start/end state predictions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example Decoding Sequences with MFC Tagger"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sentence Key: b100-28144\n",
      "\n",
      "Predicted labels:\n",
      "-----------------\n",
      "['CONJ', 'NOUN', 'NUM', '.', 'NOUN', 'NUM', '.', 'NOUN', 'NUM', '.', 'CONJ', 'NOUN', 'NUM', '.', '.', 'NOUN', '.', '.']\n",
      "\n",
      "Actual labels:\n",
      "--------------\n",
      "('CONJ', 'NOUN', 'NUM', '.', 'NOUN', 'NUM', '.', 'NOUN', 'NUM', '.', 'CONJ', 'NOUN', 'NUM', '.', '.', 'NOUN', '.', '.')\n",
      "\n",
      "\n",
      "Sentence Key: b100-23146\n",
      "\n",
      "Predicted labels:\n",
      "-----------------\n",
      "['PRON', 'VERB', 'DET', 'NOUN', 'ADP', 'ADJ', 'ADJ', 'NOUN', 'VERB', 'VERB', '.', 'ADP', 'VERB', 'DET', 'NOUN', 'ADP', 'NOUN', 'ADP', 'DET', 'NOUN', '.']\n",
      "\n",
      "Actual labels:\n",
      "--------------\n",
      "('PRON', 'VERB', 'DET', 'NOUN', 'ADP', 'ADJ', 'ADJ', 'NOUN', 'VERB', 'VERB', '.', 'ADP', 'VERB', 'DET', 'NOUN', 'ADP', 'NOUN', 'ADP', 'DET', 'NOUN', '.')\n",
      "\n",
      "\n",
      "Sentence Key: b100-35462\n",
      "\n",
      "Predicted labels:\n",
      "-----------------\n",
      "['DET', 'ADJ', 'NOUN', 'VERB', 'VERB', 'VERB', 'ADP', 'DET', 'ADJ', 'ADJ', 'NOUN', 'ADP', 'DET', 'ADJ', 'NOUN', '.', 'ADP', 'ADJ', 'NOUN', '.', 'CONJ', 'ADP', 'DET', '<MISSING>', 'ADP', 'ADJ', 'ADJ', '.', 'ADJ', '.', 'CONJ', 'ADJ', 'NOUN', 'ADP', 'ADV', 'NOUN', '.']\n",
      "\n",
      "Actual labels:\n",
      "--------------\n",
      "('DET', 'ADJ', 'NOUN', 'VERB', 'VERB', 'VERB', 'ADP', 'DET', 'ADJ', 'ADJ', 'NOUN', 'ADP', 'DET', 'ADJ', 'NOUN', '.', 'ADP', 'ADJ', 'NOUN', '.', 'CONJ', 'ADP', 'DET', 'NOUN', 'ADP', 'ADJ', 'ADJ', '.', 'ADJ', '.', 'CONJ', 'ADJ', 'NOUN', 'ADP', 'ADJ', 'NOUN', '.')\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for key in data.testing_set.keys[:3]:\n",
    "    print(\"Sentence Key: {}\\n\".format(key))\n",
    "    print(\"Predicted labels:\\n-----------------\")\n",
    "    print(simplify_decoding(data.sentences[key].words, mfc_model))\n",
    "    print()\n",
    "    print(\"Actual labels:\\n--------------\")\n",
    "    print(data.sentences[key].tags)\n",
    "    print(\"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluating Model Accuracy\n",
    "\n",
    "The function below will evaluate the accuracy of the MFC tagger on the collection of all sentences from a text corpus. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def accuracy(X, Y, model):\n",
    "    \"\"\"Calculate the prediction accuracy by using the model to decode each sequence\n",
    "    in the input X and comparing the prediction with the true labels in Y.\n",
    "    \n",
    "    The X should be an array whose first dimension is the number of sentences to test,\n",
    "    and each element of the array should be an iterable of the words in the sequence.\n",
    "    The arrays X and Y should have the exact same shape.\n",
    "    \n",
    "    X = [(\"See\", \"Spot\", \"run\"), (\"Run\", \"Spot\", \"run\", \"fast\"), ...]\n",
    "    Y = [(), (), ...]\n",
    "    \"\"\"\n",
    "    correct = total_predictions = 0\n",
    "    for observations, actual_tags in zip(X, Y):\n",
    "        \n",
    "        # The model.viterbi call in simplify_decoding will return None if the HMM\n",
    "        # raises an error (for example, if a test sentence contains a word that\n",
    "        # is out of vocabulary for the training set). Any exception counts the\n",
    "        # full sentence as an error (which makes this a conservative estimate).\n",
    "        try:\n",
    "            most_likely_tags = simplify_decoding(observations, model)\n",
    "            correct += sum(p == t for p, t in zip(most_likely_tags, actual_tags))\n",
    "        except:\n",
    "            pass\n",
    "        total_predictions += len(observations)\n",
    "    return correct / total_predictions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Evaluate the accuracy of the MFC tagger\n",
    "Run the next cell to evaluate the accuracy of the tagger on the training and test corpus."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training accuracy mfc_model: 95.71%\n",
      "testing accuracy mfc_model: 93.13%\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div class=\"alert alert-block alert-success\">Your MFC tagger accuracy looks correct!</div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mfc_training_acc = accuracy(data.training_set.X, data.training_set.Y, mfc_model)\n",
    "print(\"training accuracy mfc_model: {:.2f}%\".format(100 * mfc_training_acc))\n",
    "\n",
    "mfc_testing_acc = accuracy(data.testing_set.X, data.testing_set.Y, mfc_model)\n",
    "print(\"testing accuracy mfc_model: {:.2f}%\".format(100 * mfc_testing_acc))\n",
    "\n",
    "assert mfc_training_acc >= 0.955, \"Uh oh. Your MFC accuracy on the training set doesn't look right.\"\n",
    "assert mfc_testing_acc >= 0.925, \"Uh oh. Your MFC accuracy on the testing set doesn't look right.\"\n",
    "HTML('<div class=\"alert alert-block alert-success\">Your MFC tagger accuracy looks correct!</div>')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3: Build an HMM tagger\n",
    "---\n",
    "The HMM tagger has one hidden state for each possible tag, and parameterized by two distributions: the emission probabilties giving the conditional probability of observing a given **word** from each hidden state, and the transition probabilities giving the conditional probability of moving between **tags** during the sequence.\n",
    "\n",
    "We will also estimate the starting probability distribution (the probability of each **tag** being the first tag in a sequence), and the terminal probability distribution (the probability of each **tag** being the last tag in a sequence).\n",
    "\n",
    "The maximum likelihood estimate of these distributions can be calculated from the frequency counts as described in the following sections where you'll implement functions to count the frequencies, and finally build the model. The HMM model will make predictions according to the formula:\n",
    "\n",
    "$$t_i^n = \\underset{t_i^n}{\\mathrm{argmax}} \\prod_{i=1}^n P(w_i|t_i) P(t_i|t_{i-1})$$\n",
    "\n",
    "Refer to Speech & Language Processing [Chapter 10](https://web.stanford.edu/~jurafsky/slp3/10.pdf) for more information."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### IMPLEMENTATION: Unigram Counts\n",
    "\n",
    "Complete the function below to estimate the co-occurrence frequency of each symbol over all of the input sequences. The unigram probabilities in our HMM model are estimated from the formula below, where N is the total number of samples in the input. (You only need to compute the counts for now.)\n",
    "\n",
    "$$P(tag_1) = \\frac{C(tag_1)}{N}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div class=\"alert alert-block alert-success\">Your tag unigrams look good!</div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def unigram_counts(sequences):\n",
    "    \n",
    "    \"\"\"Return a dictionary keyed to each unique value in the input sequence list that\n",
    "    counts the number of occurrences of the value in the sequences list. The sequences\n",
    "    collection should be a 2-dimensional array.\n",
    "    \n",
    "    For example, if the tag NOUN appears 275558 times over all the input sequences,\n",
    "    then you should return a dictionary such that your_unigram_counts[NOUN] == 275558.\n",
    "    \"\"\"\n",
    "    res=[]\n",
    "    for x in sequences.values():\n",
    "        res+=x[1]\n",
    "    # TODO: Finish this function!\n",
    "    return Counter(res)\n",
    "\n",
    "# TODO: call unigram_counts with a list of tag sequences from the training set.\n",
    "\n",
    "tag_unigrams = unigram_counts(data.sentences)\n",
    "assert set(tag_unigrams.keys()) == data.training_set.tagset, \\\n",
    "       \"Uh oh. It looks like your tag counts doesn't include all the tags!\"\n",
    "assert min(tag_unigrams, key=tag_unigrams.get) == 'X', \\\n",
    "       \"Hmmm...'X' is expected to be the least common class\"\n",
    "assert max(tag_unigrams, key=tag_unigrams.get) == 'NOUN', \\\n",
    "       \"Hmmm...'NOUN' is expected to be the most common class\"\n",
    "HTML('<div class=\"alert alert-block alert-success\">Your tag unigrams look good!</div>')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### IMPLEMENTATION: Bigram Counts\n",
    "\n",
    "Complete the function below to estimate the co-occurrence frequency of each pair of symbols in each of the input sequences. These counts are used in the HMM model to estimate the bigram probability of two tags from the frequency counts according to the formula: $$P(tag_2|tag_1) = \\frac{C(tag_2|tag_1)}{C(tag_2)}$$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div class=\"alert alert-block alert-success\">Your tag bigrams look good!</div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from itertools import product\n",
    "def bigram_counts(sequences):\n",
    "    \"\"\"Return a dictionary keyed to each unique PAIR of values in the input sequences\n",
    "    list that counts the number of occurrences of pair in the sequences list. The input\n",
    "    should be a 2-dimensional array.\n",
    "    \n",
    "    For example, if the pair of tags (NOUN, VERB) appear 61582 times, then you should\n",
    "    return a dictionary such that your_bigram_counts[(NOUN, VERB)] == 61582\n",
    "    \"\"\"\n",
    "    ars = list(product(data.tagset,repeat=2))\n",
    "    voc = [x[0]+\" \"+x[1]for x  in ars]\n",
    "    count = CountVectorizer(vocabulary=voc,lowercase=False,ngram_range=(2, 2))\n",
    "    res=[]\n",
    "    for x in sequences.values():\n",
    "        res.append(' '.join(x[1]))\n",
    "    rf = count.fit_transform(res)\n",
    "    qwe =rf.sum(axis=0).tolist()[0]\n",
    "    arr = []\n",
    "    res_dict={}\n",
    "    for i,x in enumerate(count.vocabulary_.keys()):\n",
    "        res_dict[tuple(x.upper().split())]=qwe[i]\n",
    "\n",
    "    # TODO: Finish this function!\n",
    "    return res_dict\n",
    "\n",
    "# TODO: call bigram_counts with a list of tag sequences from the training set\n",
    "\n",
    "tag_bigrams = bigram_counts(data.sentences)\n",
    "\n",
    "assert len(tag_bigrams) == 144, \\\n",
    "       \"Uh oh. There should be 144 pairs of bigrams (12 tags x 12 tags)\"\n",
    "'''assert min(tag_bigrams, key=tag_bigrams.get) in [('X', 'NUM'), ('PRON', 'X')], \\\n",
    "       \"Hmmm...The least common bigram should be one of ('X', 'NUM') or ('PRON', 'X').\"'''\n",
    "assert max(tag_bigrams, key=tag_bigrams.get) in [('DET', 'NOUN')], \\\n",
    "       \"Hmmm...('DET', 'NOUN') is expected to be the most common bigram.\"\n",
    "HTML('<div class=\"alert alert-block alert-success\">Your tag bigrams look good!</div>')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### IMPLEMENTATION: Sequence Starting Counts\n",
    "Complete the code below to estimate the bigram probabilities of a sequence starting with each tag."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div class=\"alert alert-block alert-success\">Your starting tag counts look good!</div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def starting_counts(dat):\n",
    "    \"\"\"Return a dictionary keyed to each unique value in the input sequences list\n",
    "    that counts the number of occurrences where that value is at the beginning of\n",
    "    a sequence.\n",
    "    \n",
    "    For example, if 8093 sequences start with NOUN, then you should return a\n",
    "    dictionary such that your_starting_counts[NOUN] == 8093\n",
    "    \"\"\"\n",
    "    ar = list(dat.sentences.values())\n",
    "    return Counter([ar[i].tags[0] for i in range(len(dat.sentences))])\n",
    "    # TODO: Finish this function!\n",
    "    #raise NotImplementedError\n",
    "\n",
    "# TODO: Calculate the count of each tag starting a sequence\n",
    "tag_starts = starting_counts(data)# TODO: YOUR CODE HERE)\n",
    "\n",
    "assert len(tag_starts) == 12, \"Uh oh. There should be 12 tags in your dictionary.\"\n",
    "assert min(tag_starts, key=tag_starts.get) == 'X', \"Hmmm...'X' is expected to be the least common starting bigram.\"\n",
    "assert max(tag_starts, key=tag_starts.get) == 'DET', \"Hmmm...'DET' is expected to be the most common starting bigram.\"\n",
    "HTML('<div class=\"alert alert-block alert-success\">Your starting tag counts look good!</div>')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### IMPLEMENTATION: Sequence Ending Counts\n",
    "Complete the function below to estimate the bigram probabilities of a sequence ending with each tag."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div class=\"alert alert-block alert-success\">Your ending tag counts look good!</div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def ending_counts(dat):\n",
    "    \"\"\"Return a dictionary keyed to each unique value in the input sequences list\n",
    "    that counts the number of occurrences where that value is at the end of\n",
    "    a sequence.\n",
    "    \n",
    "    For example, if 18 sequences end with DET, then you should return a\n",
    "    dictionary such that your_starting_counts[DET] == 18\n",
    "    \"\"\"\n",
    "    ar = list(dat.sentences.values())\n",
    "    return Counter([ar[i].tags[-1] for i in range(len(dat.sentences))])\n",
    "    # TODO: Finish this function\n",
    "    \n",
    "\n",
    "# TODO: Calculate the count of each tag ending a sequence\n",
    "tag_ends = ending_counts(data)# TODO: YOUR CODE HERE)\n",
    "\n",
    "assert len(tag_ends) == 12, \"Uh oh. There should be 12 tags in your dictionary.\"\n",
    "assert min(tag_ends, key=tag_ends.get) in ['X', 'CONJ'], \"Hmmm...'X' or 'CONJ' should be the least common ending bigram.\"\n",
    "assert max(tag_ends, key=tag_ends.get) == '.', \"Hmmm...'.' is expected to be the most common ending bigram.\"\n",
    "HTML('<div class=\"alert alert-block alert-success\">Your ending tag counts look good!</div>')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### IMPLEMENTATION: Basic HMM Tagger\n",
    "Use the tag unigrams and bigrams calculated above to construct a hidden Markov tagger.\n",
    "\n",
    "- Add one state per tag\n",
    "    - The emission distribution at each state should be estimated with the formula: $P(w|t) = \\frac{C(t, w)}{C(t)}$\n",
    "- Add an edge from the starting state `basic_model.start` to each tag\n",
    "    - The transition probability should be estimated with the formula: $P(t|start) = \\frac{C(start, t)}{C(start)}$\n",
    "- Add an edge from each tag to the end state `basic_model.end`\n",
    "    - The transition probability should be estimated with the formula: $P(end|t) = \\frac{C(t, end)}{C(t)}$\n",
    "- Add an edge between _every_ pair of tags\n",
    "    - The transition probability should be estimated with the formula: $P(t_2|t_1) = \\frac{C(t_1, t_2)}{C(t_1)}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on HiddenMarkovModel object:\n",
      "\n",
      "class HiddenMarkovModel(pomegranate.base.GraphModel)\n",
      " |  A Hidden Markov Model\n",
      " |  \n",
      " |  A Hidden Markov Model (HMM) is a directed graphical model where nodes are\n",
      " |  hidden states which contain an observed emission distribution and edges\n",
      " |  contain the probability of transitioning from one hidden state to another.\n",
      " |  HMMs allow you to tag each observation in a variable length sequence with\n",
      " |  the most likely hidden state according to the model.\n",
      " |  \n",
      " |  Parameters\n",
      " |  ----------\n",
      " |  name : str, optional\n",
      " |      The name of the model. Default is None.\n",
      " |  \n",
      " |  start : State, optional\n",
      " |      An optional state to force the model to start in. Default is None.\n",
      " |  \n",
      " |  end : State, optional\n",
      " |      An optional state to force the model to end in. Default is None.\n",
      " |  \n",
      " |  Attributes\n",
      " |  ----------\n",
      " |  start : State\n",
      " |      A state object corresponding to the initial start of the model\n",
      " |  \n",
      " |  end : State\n",
      " |      A state object corresponding to the forced end of the model\n",
      " |  \n",
      " |  start_index : int\n",
      " |      The index of the start object in the state list\n",
      " |  \n",
      " |  end_index : int\n",
      " |      The index of the end object in the state list\n",
      " |  \n",
      " |  silent_start : int\n",
      " |      The index of the beginning of the silent states in the state list\n",
      " |  \n",
      " |  states : list\n",
      " |      The list of all states in the model, with silent states at the end\n",
      " |  \n",
      " |  Examples\n",
      " |  --------\n",
      " |  >>> from pomegranate import *\n",
      " |  >>> d1 = DiscreteDistribution({'A' : 0.35, 'C' : 0.20, 'G' : 0.05, 'T' : 0.40})\n",
      " |  >>> d2 = DiscreteDistribution({'A' : 0.25, 'C' : 0.25, 'G' : 0.25, 'T' : 0.25})\n",
      " |  >>> d3 = DiscreteDistribution({'A' : 0.10, 'C' : 0.40, 'G' : 0.40, 'T' : 0.10})\n",
      " |  >>>\n",
      " |  >>> s1 = State(d1, name=\"s1\")\n",
      " |  >>> s2 = State(d2, name=\"s2\")\n",
      " |  >>> s3 = State(d3, name=\"s3\")\n",
      " |  >>>\n",
      " |  >>> model = HiddenMarkovModel('example')\n",
      " |  >>> model.add_states([s1, s2, s3])\n",
      " |  >>> model.add_transition(model.start, s1, 0.90)\n",
      " |  >>> model.add_transition(model.start, s2, 0.10)\n",
      " |  >>> model.add_transition(s1, s1, 0.80)\n",
      " |  >>> model.add_transition(s1, s2, 0.20)\n",
      " |  >>> model.add_transition(s2, s2, 0.90)\n",
      " |  >>> model.add_transition(s2, s3, 0.10)\n",
      " |  >>> model.add_transition(s3, s3, 0.70)\n",
      " |  >>> model.add_transition(s3, model.end, 0.30)\n",
      " |  >>> model.bake()\n",
      " |  >>>\n",
      " |  >>> print(model.log_probability(list('ACGACTATTCGAT')))\n",
      " |  -22.73896159971087\n",
      " |  >>> print(\", \".join(state.name for i, state in model.viterbi(list('ACGACTATTCGAT'))[1]))\n",
      " |  example-start, s1, s2, s2, s2, s2, s2, s2, s2, s2, s2, s2, s2, s3, example-end\n",
      " |  \n",
      " |  Method resolution order:\n",
      " |      HiddenMarkovModel\n",
      " |      pomegranate.base.GraphModel\n",
      " |      pomegranate.base.Model\n",
      " |      builtins.object\n",
      " |  \n",
      " |  Methods defined here:\n",
      " |  \n",
      " |  __getstate__(...)\n",
      " |      Return model representation in a dictionary.\n",
      " |  \n",
      " |  __init__(self, /, *args, **kwargs)\n",
      " |      Initialize self.  See help(type(self)) for accurate signature.\n",
      " |  \n",
      " |  __reduce__(...)\n",
      " |      Helper for pickle.\n",
      " |  \n",
      " |  __setstate__(...)\n",
      " |      Deserialize object for unpickling.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      state :\n",
      " |          The model state, (see `__reduce__()` documentation from the pickle protocol).\n",
      " |  \n",
      " |  add_model(...)\n",
      " |      Add the states and edges of another model to this model.\n",
      " |      \n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      other : HiddenMarkovModel\n",
      " |          The other model to add\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      None\n",
      " |  \n",
      " |  add_state(...)\n",
      " |      Add a state to the given model.\n",
      " |      \n",
      " |      The state must not already be in the model, nor may it be part of any\n",
      " |      other model that will eventually be combined with this one.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      state : State\n",
      " |          A state object to be added to the model.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      None\n",
      " |  \n",
      " |  add_states(...)\n",
      " |      Add multiple states to the model at the same time.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      states : list or generator\n",
      " |          Either a list of states which are entered sequentially, or just\n",
      " |          comma separated values, for example model.add_states(a, b, c, d).\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      None\n",
      " |  \n",
      " |  add_transition(...)\n",
      " |      Add a transition from state a to state b.\n",
      " |      \n",
      " |      Add a transition from state a to state b with the given (non-log)\n",
      " |      probability. Both states must be in the HMM already. self.start and\n",
      " |      self.end are valid arguments here. Probabilities will be normalized\n",
      " |      such that every node has edges summing to 1. leaving that node, but\n",
      " |      only when the model is baked. Psueodocounts are allowed as a way of\n",
      " |      using edge-specific pseudocounts for training.\n",
      " |      \n",
      " |      By specifying a group as a string, you can tie edges together by giving\n",
      " |      them the same group. This means that a transition across one edge in the\n",
      " |      group counts as a transition across all edges in terms of training.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      a : State\n",
      " |          The state that the edge originates from\n",
      " |      \n",
      " |      b : State\n",
      " |          The state that the edge goes to\n",
      " |      \n",
      " |      probability : double\n",
      " |          The probability of transitioning from state a to state b in [0, 1]\n",
      " |      \n",
      " |      pseudocount : double, optional\n",
      " |          The pseudocount to use for this specific edge if using edge\n",
      " |          pseudocounts for training. Defaults to the probability. Default\n",
      " |          is None.\n",
      " |      \n",
      " |      group : str, optional\n",
      " |          The name of the group of edges to tie together during training. If\n",
      " |          groups are used, then a transition across any one edge counts as a\n",
      " |          transition across all edges. Default is None.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      None\n",
      " |  \n",
      " |  add_transitions(...)\n",
      " |      Add many transitions at the same time,\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      a : State or list\n",
      " |          Either a state or a list of states where the edges originate.\n",
      " |      \n",
      " |      b : State or list\n",
      " |          Either a state or a list of states where the edges go to.\n",
      " |      \n",
      " |      probabilities : list\n",
      " |          The probabilities associated with each transition.\n",
      " |      \n",
      " |      pseudocounts : list, optional\n",
      " |          The pseudocounts associated with each transition. Default is None.\n",
      " |      \n",
      " |      groups : list, optional\n",
      " |          The groups of each edge. Default is None.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      None\n",
      " |      \n",
      " |      Examples\n",
      " |      --------\n",
      " |      >>> model.add_transitions([model.start, s1], [s1, model.end], [1., 1.])\n",
      " |      >>> model.add_transitions([model.start, s1, s2, s3], s4, [0.2, 0.4, 0.3, 0.9])\n",
      " |      >>> model.add_transitions(model.start, [s1, s2, s3], [0.6, 0.2, 0.05])\n",
      " |  \n",
      " |  backward(...)\n",
      " |      Run the backward algorithm on the sequence.\n",
      " |      \n",
      " |      Calculate the probability of each observation being aligned to each\n",
      " |      state by going backward through a sequence. Returns the full backward\n",
      " |      matrix. Each index i, j corresponds to the sum-of-all-paths log\n",
      " |      probability of starting at the end of the sequence, and aligning\n",
      " |      observations to hidden states in such a manner that observation i was\n",
      " |      aligned to hidden state j. Uses row normalization to dynamically scale\n",
      " |      each row to prevent underflow errors.\n",
      " |      \n",
      " |      If the sequence is impossible, will return a matrix of nans.\n",
      " |      \n",
      " |      See also:\n",
      " |          - Silent state handling taken from p. 71 of \"Biological\n",
      " |      Sequence Analysis\" by Durbin et al., and works for anything which\n",
      " |      does not have loops of silent states.\n",
      " |          - Row normalization technique explained by\n",
      " |      http://www.cs.sjsu.edu/~stamp/RUA/HMM.pdf on p. 14.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      sequence : array-like\n",
      " |          An array (or list) of observations.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      matrix : array-like, shape (len(sequence), n_states)\n",
      " |          The probability of aligning the sequences to states in a backward\n",
      " |          fashion.\n",
      " |  \n",
      " |  bake(...)\n",
      " |      Finalize the topology of the model.\n",
      " |      \n",
      " |      Finalize the topology of the model and assign a numerical index to\n",
      " |      every state. This method must be called before any of the probability-\n",
      " |      calculating methods.\n",
      " |      \n",
      " |      This fills in self.states (a list of all states in order) and\n",
      " |      self.transition_log_probabilities (log probabilities for transitions),\n",
      " |      as well as self.start_index and self.end_index, and self.silent_start\n",
      " |      (the index of the first silent state).\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      verbose : bool, optional\n",
      " |          Return a log of changes made to the model during normalization\n",
      " |          or merging. Default is False.\n",
      " |      \n",
      " |      merge : \"None\", \"Partial, \"All\"\n",
      " |          Merging has three options:\n",
      " |          \"None\": No modifications will be made to the model.\n",
      " |          \"Partial\": A silent state which only has a probability 1 transition\n",
      " |              to another silent state will be merged with that silent state.\n",
      " |              This means that if silent state \"S1\" has a single transition\n",
      " |              to silent state \"S2\", that all transitions to S1 will now go\n",
      " |              to S2, with the same probability as before, and S1 will be\n",
      " |              removed from the model.\n",
      " |          \"All\": A silent state with a probability 1 transition to any other\n",
      " |              state, silent or symbol emitting, will be merged in the manner\n",
      " |              described above. In addition, any orphan states will be removed\n",
      " |              from the model. An orphan state is a state which does not have\n",
      " |              any transitions to it OR does not have any transitions from it,\n",
      " |              except for the start and end of the model. This will iteratively\n",
      " |              remove orphan chains from the model. This is sometimes desirable,\n",
      " |              as all states should have both a transition in to get to that\n",
      " |              state, and a transition out, even if it is only to itself. If\n",
      " |              the state does not have either, the HMM will likely not work as\n",
      " |              intended.\n",
      " |          Default is 'All'.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      None\n",
      " |  \n",
      " |  clear_summaries(...)\n",
      " |      Clear the summary statistics stored in the object.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      None\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      None\n",
      " |  \n",
      " |  concatenate(...)\n",
      " |      Concatenate this model to another model.\n",
      " |      \n",
      " |      Concatenate this model to another model in such a way that a single\n",
      " |      probability 1 edge is added between self.end and other.start. Rename\n",
      " |      all other states appropriately by adding a suffix or prefix if needed.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      other : HiddenMarkovModel\n",
      " |          The other model to concatenate\n",
      " |      \n",
      " |      suffix : str, optional\n",
      " |          Add the suffix to the end of all state names in the other model.\n",
      " |          Default is ''.\n",
      " |      \n",
      " |      prefix : str, optional\n",
      " |          Add the prefix to the beginning of all state names in the other\n",
      " |          model. Default is ''.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      None\n",
      " |  \n",
      " |  copy(...)\n",
      " |      Returns a deep copy of the HMM.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      None\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      model : HiddenMarkovModel\n",
      " |          A deep copy of the model with entirely new objects.\n",
      " |  \n",
      " |  dense_transition_matrix(...)\n",
      " |      Returns the dense transition matrix.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      None\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      matrix : numpy.ndarray, shape (n_states, n_states)\n",
      " |          A dense transition matrix, containing the log probability\n",
      " |          of transitioning from each state to each other state.\n",
      " |  \n",
      " |  draw(...)\n",
      " |  \n",
      " |  fit(...)\n",
      " |      Fit the model to data using either Baum-Welch, Viterbi, or supervised training.\n",
      " |      \n",
      " |      Given a list of sequences, performs re-estimation on the model\n",
      " |      parameters. The two supported algorithms are \"baum-welch\", \"viterbi\",\n",
      " |      and \"labeled\", indicating their respective algorithm. \"labeled\"\n",
      " |      corresponds to supervised learning that requires passing in a matching\n",
      " |      list of labels for each symbol seen in the sequences.\n",
      " |      \n",
      " |      Training supports a wide variety of other options including using\n",
      " |      edge pseudocounts and either edge or distribution inertia.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      sequences : array-like\n",
      " |          An array of some sort (list, numpy.ndarray, tuple..) of sequences,\n",
      " |          where each sequence is a numpy array, which is 1 dimensional if\n",
      " |          the HMM is a one dimensional array, or multidimensional if the HMM\n",
      " |          supports multiple dimensions.\n",
      " |      \n",
      " |      weights : array-like or None, optional\n",
      " |          An array of weights, one for each sequence to train on. If None,\n",
      " |          all sequences are equally weighted. Default is None.\n",
      " |      \n",
      " |      labels : array-like or None, optional\n",
      " |          An array of state labels for each sequence. This is only used in\n",
      " |          'labeled' training. If used this must be comprised of n lists where\n",
      " |          n is the number of sequences to train on, and each of those lists\n",
      " |          must have one label per observation. A None in this list corresponds\n",
      " |          to no labels for the entire sequence and triggers semi-supervised\n",
      " |          learning, where the labeled sequences are summarized using labeled\n",
      " |          fitting and the unlabeled are summarized using the specified algorithm.\n",
      " |          Default is None.\n",
      " |      \n",
      " |      stop_threshold : double, optional\n",
      " |          The threshold the improvement ratio of the models log probability\n",
      " |          in fitting the scores. Default is 1e-9.\n",
      " |      \n",
      " |      min_iterations : int, optional\n",
      " |          The minimum number of iterations to run Baum-Welch training for.\n",
      " |          Default is 0.\n",
      " |      \n",
      " |      max_iterations : int, optional\n",
      " |          The maximum number of iterations to run Baum-Welch training for.\n",
      " |          Default is 1e8.\n",
      " |      \n",
      " |      algorithm : 'baum-welch', 'viterbi', 'labeled'\n",
      " |          The training algorithm to use. Baum-Welch uses the forward-backward\n",
      " |          algorithm to train using a version of structured EM. Viterbi\n",
      " |          iteratively runs the sequences through the Viterbi algorithm and\n",
      " |          then uses hard assignments of observations to states using that.\n",
      " |          Default is 'baum-welch'. Labeled training requires that labels\n",
      " |          are provided for each observation in each sequence.\n",
      " |      \n",
      " |      pseudocount : double, optional\n",
      " |          A pseudocount to add to both transitions and emissions. If supplied,\n",
      " |          it will override both transition_pseudocount and emission_pseudocount\n",
      " |          in the same way that specifying `inertia` will override both\n",
      " |          `edge_inertia` and `distribution_inertia`. Default is None.\n",
      " |      \n",
      " |      transition_pseudocount : double, optional\n",
      " |          A pseudocount to add to all transitions to add a prior to the\n",
      " |          MLE estimate of the transition probability. Default is 0.\n",
      " |      \n",
      " |      emission_pseudocount : double, optional\n",
      " |          A pseudocount to add to the emission of each distribution. This\n",
      " |          effectively smoothes the states to prevent 0. probability symbols\n",
      " |          if they don't happen to occur in the data. Only effects hidden\n",
      " |          Markov models defined over discrete distributions. Default is 0.\n",
      " |      \n",
      " |      use_pseudocount : bool, optional\n",
      " |          Whether to use the pseudocounts defined in the `add_edge` method\n",
      " |          for edge-specific pseudocounts when updating the transition\n",
      " |          probability parameters. Does not effect the `transition_pseudocount`\n",
      " |          and `emission_pseudocount` parameters, but can be used in addition\n",
      " |          to them. Default is False.\n",
      " |      \n",
      " |      inertia : double or None, optional, range [0, 1]\n",
      " |          If double, will set both edge_inertia and distribution_inertia to\n",
      " |          be that value. If None, will not override those values. Default is\n",
      " |          None.\n",
      " |      \n",
      " |      edge_inertia : bool, optional, range [0, 1]\n",
      " |          Whether to use inertia when updating the transition probability\n",
      " |          parameters. Default is 0.0.\n",
      " |      \n",
      " |      distribution_inertia : double, optional, range [0, 1]\n",
      " |          Whether to use inertia when updating the distribution parameters.\n",
      " |          Default is 0.0.\n",
      " |      \n",
      " |      batch_size : int or None, optional\n",
      " |          The number of samples in a batch to summarize on. This controls\n",
      " |          the size of the set sent to `summarize` and so does not make the\n",
      " |          update any less exact. This is useful when training on a memory\n",
      " |          map and cannot load all the data into memory. If set to None,\n",
      " |          batch_size is 1 / n_jobs. Default is None.\n",
      " |      \n",
      " |      batches_per_epoch : int or None, optional\n",
      " |          The number of batches in an epoch. This is the number of batches to\n",
      " |          summarize before calling `from_summaries` and updating the model\n",
      " |          parameters. This allows one to do minibatch updates by updating the\n",
      " |          model parameters before setting the full dataset. If set to None,\n",
      " |          uses the full dataset. Default is None.\n",
      " |      \n",
      " |      lr_decay : double, optional, positive\n",
      " |          The step size decay as a function of the number of iterations.\n",
      " |          Functionally, this sets the inertia to be (2+k)^{-lr_decay}\n",
      " |          where k is the number of iterations. This causes initial\n",
      " |          iterations to have more of an impact than later iterations,\n",
      " |          and is frequently used in minibatch learning. This value is\n",
      " |          suggested to be between 0.5 and 1. Default is 0, meaning no\n",
      " |          decay.\n",
      " |      \n",
      " |      callbacks : list, optional\n",
      " |          A list of callback objects that describe functionality that should\n",
      " |          be undertaken over the course of training.\n",
      " |      \n",
      " |      return_history : bool, optional\n",
      " |          Whether to return the history during training as well as the model.\n",
      " |      \n",
      " |      verbose : bool, optional\n",
      " |          Whether to print the improvement in the model fitting at each\n",
      " |          iteration. Default is True.\n",
      " |      \n",
      " |      n_jobs : int, optional\n",
      " |          The number of threads to use when performing training. This\n",
      " |          leads to exact updates. Default is 1.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      improvement : double\n",
      " |          The total improvement in fitting the model to the data\n",
      " |  \n",
      " |  forward(...)\n",
      " |      Run the forward algorithm on the sequence.\n",
      " |      \n",
      " |      Calculate the probability of each observation being aligned to each\n",
      " |      state by going forward through a sequence. Returns the full forward\n",
      " |      matrix. Each index i, j corresponds to the sum-of-all-paths log\n",
      " |      probability of starting at the beginning of the sequence, and aligning\n",
      " |      observations to hidden states in such a manner that observation i was\n",
      " |      aligned to hidden state j. Uses row normalization to dynamically scale\n",
      " |      each row to prevent underflow errors.\n",
      " |      \n",
      " |      If the sequence is impossible, will return a matrix of nans.\n",
      " |      \n",
      " |      See also:\n",
      " |          - Silent state handling taken from p. 71 of \"Biological\n",
      " |      Sequence Analysis\" by Durbin et al., and works for anything which\n",
      " |      does not have loops of silent states.\n",
      " |          - Row normalization technique explained by\n",
      " |      http://www.cs.sjsu.edu/~stamp/RUA/HMM.pdf on p. 14.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      sequence : array-like\n",
      " |          An array (or list) of observations.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      matrix : array-like, shape (len(sequence), n_states)\n",
      " |          The probability of aligning the sequences to states in a forward\n",
      " |          fashion.\n",
      " |  \n",
      " |  forward_backward(...)\n",
      " |      Run the forward-backward algorithm on the sequence.\n",
      " |      \n",
      " |      This algorithm returns an emission matrix and a transition matrix. The\n",
      " |      emission matrix returns the normalized probability that each each state\n",
      " |      generated that emission given both the symbol and the entire sequence.\n",
      " |      The transition matrix returns the expected number of times that a\n",
      " |      transition is used.\n",
      " |      \n",
      " |      If the sequence is impossible, will return (None, None)\n",
      " |      \n",
      " |      See also:\n",
      " |          - Forward and backward algorithm implementations. A comprehensive\n",
      " |          description of the forward, backward, and forward-background\n",
      " |          algorithm is here:\n",
      " |          http://en.wikipedia.org/wiki/Forward%E2%80%93backward_algorithm\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      sequence : array-like\n",
      " |          An array (or list) of observations.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      emissions : array-like, shape (len(sequence), n_nonsilent_states)\n",
      " |          The normalized probabilities of each state generating each emission.\n",
      " |      \n",
      " |      transitions : array-like, shape (n_states, n_states)\n",
      " |          The expected number of transitions across each edge in the model.\n",
      " |  \n",
      " |  free_bake_buffers(...)\n",
      " |  \n",
      " |  freeze_distributions(...)\n",
      " |      Freeze all the distributions in model.\n",
      " |      \n",
      " |      Upon training only edges will be updated. The parameters of\n",
      " |      distributions will not be affected.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      None\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      None\n",
      " |  \n",
      " |  from_summaries(...)\n",
      " |      Fit the model to the stored summary statistics.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      inertia : double or None, optional\n",
      " |          The inertia to use for both edges and distributions without\n",
      " |          needing to set both of them. If None, use the values passed\n",
      " |          in to those variables. Default is None.\n",
      " |      \n",
      " |      pseudocount : double, optional\n",
      " |          A pseudocount to add to both transitions and emissions. If supplied,\n",
      " |          it will override both transition_pseudocount and emission_pseudocount\n",
      " |          in the same way that specifying `inertia` will override both\n",
      " |          `edge_inertia` and `distribution_inertia`. Default is None.\n",
      " |      \n",
      " |      transition_pseudocount : double, optional\n",
      " |          A pseudocount to add to all transitions to add a prior to the\n",
      " |          MLE estimate of the transition probability. Default is 0.\n",
      " |      \n",
      " |      emission_pseudocount : double, optional\n",
      " |          A pseudocount to add to the emission of each distribution. This\n",
      " |          effectively smoothes the states to prevent 0. probability symbols\n",
      " |          if they don't happen to occur in the data. Only effects hidden\n",
      " |          Markov models defined over discrete distributions. Default is 0.\n",
      " |      \n",
      " |      use_pseudocount : bool, optional\n",
      " |          Whether to use the pseudocounts defined in the `add_edge` method\n",
      " |          for edge-specific pseudocounts when updating the transition\n",
      " |          probability parameters. Does not effect the `transition_pseudocount`\n",
      " |          and `emission_pseudocount` parameters, but can be used in addition\n",
      " |          to them. Default is False.\n",
      " |      \n",
      " |      edge_inertia : bool, optional, range [0, 1]\n",
      " |          Whether to use inertia when updating the transition probability\n",
      " |          parameters. Default is 0.0.\n",
      " |      \n",
      " |      distribution_inertia : double, optional, range [0, 1]\n",
      " |          Whether to use inertia when updating the distribution parameters.\n",
      " |          Default is 0.0.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      None\n",
      " |  \n",
      " |  log_probability(...)\n",
      " |      Calculate the log probability of a single sequence.\n",
      " |      \n",
      " |      If a path is provided, calculate the log probability of that sequence\n",
      " |      given the path.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      sequence : array-like\n",
      " |          Return the array of observations in a single sequence of data\n",
      " |      \n",
      " |      check_input : bool, optional\n",
      " |          Check to make sure that all emissions fall under the support of\n",
      " |          the emission distributions. Default is True.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      logp : double\n",
      " |          The log probability of the sequence\n",
      " |  \n",
      " |  maximum_a_posteriori(...)\n",
      " |      Run posterior decoding on the sequence.\n",
      " |      \n",
      " |      MAP decoding is an alternative to viterbi decoding, which returns the\n",
      " |      most likely state for each observation, based on the forward-backward\n",
      " |      algorithm. This is also called posterior decoding. This method is\n",
      " |      described on p. 14 of http://ai.stanford.edu/~serafim/CS262_2007/\n",
      " |      notes/lecture5.pdf\n",
      " |      \n",
      " |      WARNING: This may produce impossible sequences.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      sequence : array-like\n",
      " |          An array (or list) of observations.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      logp : double\n",
      " |          The log probability of the sequence under the Viterbi path\n",
      " |      \n",
      " |      path : list of tuples\n",
      " |          Tuples of (state index, state object) of the states along the\n",
      " |          posterior path.\n",
      " |  \n",
      " |  plot(...)\n",
      " |      Draw this model's graph using NetworkX and matplotlib.\n",
      " |      \n",
      " |      Note that this relies on networkx's built-in graphing capabilities (and\n",
      " |      not Graphviz) and thus can't draw self-loops.\n",
      " |      \n",
      " |      See networkx.draw_networkx() for the keywords you can pass in.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      precision : int, optional\n",
      " |          The precision with which to round edge probabilities.\n",
      " |          Default is 4.\n",
      " |      \n",
      " |      **kwargs : any\n",
      " |          The arguments to pass into networkx.draw_networkx()\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      None\n",
      " |  \n",
      " |  predict(...)\n",
      " |      Calculate the most likely state for each observation.\n",
      " |      \n",
      " |      This can be either the Viterbi algorithm or maximum a posteriori. It\n",
      " |      returns the probability of the sequence under that state sequence and\n",
      " |      the actual state sequence.\n",
      " |      \n",
      " |      This is a sklearn wrapper for the Viterbi and maximum_a_posteriori methods.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      sequence : array-like\n",
      " |          An array (or list) of observations.\n",
      " |      \n",
      " |      algorithm : \"map\", \"viterbi\"\n",
      " |          The algorithm with which to decode the sequence\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      path : list of integers\n",
      " |          A list of the ids of states along the MAP or the Viterbi path.\n",
      " |  \n",
      " |  predict_log_proba(...)\n",
      " |      Calculate the state log probabilities for each observation in the sequence.\n",
      " |      \n",
      " |      Run the forward-backward algorithm on the sequence and return the emission\n",
      " |      matrix. This is the log normalized probability that each each state\n",
      " |      generated that emission given both the symbol and the entire sequence.\n",
      " |      \n",
      " |      This is a sklearn wrapper for the forward backward algorithm.\n",
      " |      \n",
      " |      See also:\n",
      " |          - Forward and backward algorithm implementations. A comprehensive\n",
      " |          description of the forward, backward, and forward-background\n",
      " |          algorithm is here:\n",
      " |          http://en.wikipedia.org/wiki/Forward%E2%80%93backward_algorithm\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      sequence : array-like\n",
      " |          An array (or list) of observations.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      emissions : array-like, shape (len(sequence), n_nonsilent_states)\n",
      " |          The log normalized probabilities of each state generating each emission.\n",
      " |  \n",
      " |  predict_proba(...)\n",
      " |      Calculate the state probabilities for each observation in the sequence.\n",
      " |      \n",
      " |      Run the forward-backward algorithm on the sequence and return the emission\n",
      " |      matrix. This is the normalized probability that each each state\n",
      " |      generated that emission given both the symbol and the entire sequence.\n",
      " |      \n",
      " |      This is a sklearn wrapper for the forward backward algorithm.\n",
      " |      \n",
      " |      See also:\n",
      " |          - Forward and backward algorithm implementations. A comprehensive\n",
      " |          description of the forward, backward, and forward-background\n",
      " |          algorithm is here:\n",
      " |          http://en.wikipedia.org/wiki/Forward%E2%80%93backward_algorithm\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      sequence : array-like\n",
      " |          An array (or list) of observations.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      emissions : array-like, shape (len(sequence), n_nonsilent_states)\n",
      " |          The normalized probabilities of each state generating each emission.\n",
      " |  \n",
      " |  sample(...)\n",
      " |      Generate a sequence from the model.\n",
      " |      \n",
      " |      Returns the sequence generated, as a list of emitted items. The\n",
      " |      model must have been baked first in order to run this method.\n",
      " |      \n",
      " |      If a length is specified and the HMM is infinite (no edges to the\n",
      " |      end state), then that number of samples will be randomly generated.\n",
      " |      If the length is specified and the HMM is finite, the method will\n",
      " |      attempt to generate a prefix of that length. Currently it will force\n",
      " |      itself to not take an end transition unless that is the only path,\n",
      " |      making it not a true random sample on a finite model.\n",
      " |      \n",
      " |      WARNING: If the HMM has no explicit end state, must specify a length\n",
      " |      to use.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      n : int or None, optional\n",
      " |          The number of samples to generate. If None, return only one sample.\n",
      " |      \n",
      " |      length : int, optional\n",
      " |          Generate a sequence with a maximal length of this size. Used if\n",
      " |          you have no explicit end state. Default is 0.\n",
      " |      \n",
      " |      path : bool, optional\n",
      " |          Return the path of hidden states in addition to the emissions. If\n",
      " |          true will return a tuple of (sample, path). Default is False.\n",
      " |      \n",
      " |      random_state : int, numpy.random.RandomState, or None\n",
      " |          The random state used for generating samples. If set to none, a\n",
      " |          random seed will be used. If set to either an integer or a\n",
      " |          random seed, will produce deterministic outputs.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      sample : list or tuple\n",
      " |          If path is true, return a tuple of (sample, path), otherwise return\n",
      " |          just the samples.\n",
      " |  \n",
      " |  summarize(...)\n",
      " |      Summarize data into stored sufficient statistics for out-of-core\n",
      " |      training. Only implemented for Baum-Welch training since Viterbi\n",
      " |      is less memory intensive.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      sequences : array-like\n",
      " |          An array of some sort (list, numpy.ndarray, tuple..) of sequences,\n",
      " |          where each sequence is a numpy array, which is 1 dimensional if\n",
      " |          the HMM is a one dimensional array, or multidimensional if the HMM\n",
      " |          supports multiple dimensions.\n",
      " |      \n",
      " |      weights : array-like or None, optional\n",
      " |          An array of weights, one for each sequence to train on. If None,\n",
      " |          all sequences are equally weighted. Default is None.\n",
      " |      \n",
      " |      labels : array-like or None, optional\n",
      " |          An array of state labels for each sequence. This is only used in\n",
      " |          'labeled' training. If used this must be comprised of n lists where\n",
      " |          n is the number of sequences to train on, and each of those lists\n",
      " |          must have one label per observation. Default is None.\n",
      " |      \n",
      " |      algorithm : 'baum-welch', 'viterbi', 'labeled'\n",
      " |          The training algorithm to use. Baum-Welch uses the forward-backward\n",
      " |          algorithm to train using a version of structured EM. Viterbi\n",
      " |          iteratively runs the sequences through the Viterbi algorithm and\n",
      " |          then uses hard assignments of observations to states using that.\n",
      " |          Default is 'baum-welch'. Labeled training requires that labels\n",
      " |          are provided for each observation in each sequence.\n",
      " |      \n",
      " |      check_input : bool, optional\n",
      " |          Check the input. This casts the input sequences as numpy arrays,\n",
      " |          and converts non-numeric inputs into numeric inputs for faster\n",
      " |          processing later. Default is True.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      logp : double\n",
      " |          The log probability of the sequences.\n",
      " |  \n",
      " |  thaw_distributions(...)\n",
      " |      Thaw all distributions in the model.\n",
      " |      \n",
      " |      Upon training distributions will be updated again.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      None\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      None\n",
      " |  \n",
      " |  to_json(...)\n",
      " |      Serialize the model to a JSON.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      separators : tuple, optional\n",
      " |          The two separators to pass to the json.dumps function for formatting.\n",
      " |      \n",
      " |      indent : int, optional\n",
      " |          The indentation to use at each level. Passed to json.dumps for\n",
      " |          formatting.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      json : str\n",
      " |          A properly formatted JSON object.\n",
      " |  \n",
      " |  viterbi(...)\n",
      " |      Run the Viteri algorithm on the sequence.\n",
      " |      \n",
      " |      Run the Viterbi algorithm on the sequence given the model. This finds\n",
      " |      the ML path of hidden states given the sequence. Returns a tuple of the\n",
      " |      log probability of the ML path, or (-inf, None) if the sequence is\n",
      " |      impossible under the model. If a path is returned, it is a list of\n",
      " |      tuples of the form (sequence index, state object).\n",
      " |      \n",
      " |      This is fundamentally the same as the forward algorithm using max\n",
      " |      instead of sum, except the traceback is more complicated, because\n",
      " |      silent states in the current step can trace back to other silent states\n",
      " |      in the current step as well as states in the previous step.\n",
      " |      \n",
      " |      See also:\n",
      " |          - Viterbi implementation described well in the wikipedia article\n",
      " |          http://en.wikipedia.org/wiki/Viterbi_algorithm\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      sequence : array-like\n",
      " |          An array (or list) of observations.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      logp : double\n",
      " |          The log probability of the sequence under the Viterbi path\n",
      " |      \n",
      " |      path : list of tuples\n",
      " |          Tuples of (state index, state object) of the states along the\n",
      " |          Viterbi path.\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Class methods defined here:\n",
      " |  \n",
      " |  from_json(...) from builtins.type\n",
      " |      Read in a serialized model and return the appropriate classifier.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      s : str\n",
      " |          A JSON formatted string containing the file.\n",
      " |      Returns\n",
      " |      -------\n",
      " |      model : object\n",
      " |          A properly initialized and baked model.\n",
      " |  \n",
      " |  from_matrix(...) from builtins.type\n",
      " |      Create a model from a more standard matrix format.\n",
      " |      \n",
      " |      Take in a 2D matrix of floats of size n by n, which are the transition\n",
      " |      probabilities to go from any state to any other state. May also take in\n",
      " |      a list of length n representing the names of these nodes, and a model\n",
      " |      name. Must provide the matrix, and a list of size n representing the\n",
      " |      distribution you wish to use for that state, a list of size n indicating\n",
      " |      the probability of starting in a state, and a list of size n indicating\n",
      " |      the probability of ending in a state.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      transition_probabilities : array-like, shape (n_states, n_states)\n",
      " |          The probabilities of each state transitioning to each other state.\n",
      " |      \n",
      " |      distributions : array-like, shape (n_states)\n",
      " |          The distributions for each state. Silent states are indicated by\n",
      " |          using None instead of a distribution object.\n",
      " |      \n",
      " |      starts : array-like, shape (n_states)\n",
      " |          The probabilities of starting in each of the states.\n",
      " |      \n",
      " |      ends : array-like, shape (n_states), optional\n",
      " |          If passed in, the probabilities of ending in each of the states.\n",
      " |          If ends is None, then assumes the model has no explicit end\n",
      " |          state. Default is None.\n",
      " |      \n",
      " |      state_names : array-like, shape (n_states), optional\n",
      " |          The name of the states. If None is passed in, default names are\n",
      " |          generated. Default is None\n",
      " |      \n",
      " |      name : str, optional\n",
      " |          The name of the model. Default is None\n",
      " |      \n",
      " |      verbose : bool, optional\n",
      " |          The verbose parameter for the underlying bake method. Default is False.\n",
      " |      \n",
      " |      merge : 'None', 'Partial', 'All', optional\n",
      " |          The merge parameter for the underlying bake method. Default is All\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      model : HiddenMarkovModel\n",
      " |          The baked model ready to go.\n",
      " |      \n",
      " |      Examples\n",
      " |      --------\n",
      " |      matrix = [[0.4, 0.5], [0.4, 0.5]]\n",
      " |      distributions = [NormalDistribution(1, .5), NormalDistribution(5, 2)]\n",
      " |      starts = [1., 0.]\n",
      " |      ends = [.1., .1]\n",
      " |      state_names= [\"A\", \"B\"]\n",
      " |      \n",
      " |      model = Model.from_matrix(matrix, distributions, starts, ends,\n",
      " |          state_names, name=\"test_model\")\n",
      " |  \n",
      " |  from_samples(...) from builtins.type\n",
      " |      Learn the transitions and emissions of a model directly from data.\n",
      " |      \n",
      " |      This method will learn both the transition matrix, emission distributions,\n",
      " |      and start probabilities for each state. This will only return a dense\n",
      " |      graph without any silent states or explicit transitions to an end state.\n",
      " |      Currently all components must be defined as the same distribution, but\n",
      " |      soon this restriction will be removed.\n",
      " |      \n",
      " |      If learning a multinomial HMM over discrete characters, the initial\n",
      " |      emisison probabilities are initialized randomly. If learning a\n",
      " |      continuous valued HMM, such as a Gaussian HMM, then kmeans clustering\n",
      " |      is used first to identify initial clusters.\n",
      " |      \n",
      " |      Regardless of the type of model, the transition matrix and start\n",
      " |      probabilities are initialized uniformly. Then the specified learning\n",
      " |      algorithm (Baum-Welch recommended) is used to refine the parameters\n",
      " |      of the model.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      distribution : callable\n",
      " |          The emission distribution of the components of the model.\n",
      " |      \n",
      " |      n_components : int\n",
      " |          The number of states (or components) to initialize.\n",
      " |      \n",
      " |      X : array-like\n",
      " |          An array of some sort (list, numpy.ndarray, tuple..) of sequences,\n",
      " |          where each sequence is a numpy array, which is 1 dimensional if\n",
      " |          the HMM is a one dimensional array, or multidimensional if the HMM\n",
      " |          supports multiple dimensions.\n",
      " |      \n",
      " |      weights : array-like or None, optional\n",
      " |          An array of weights, one for each sequence to train on. If None,\n",
      " |          all sequences are equally weighted. Default is None.\n",
      " |      \n",
      " |      labels : array-like or None, optional\n",
      " |          An array of state labels for each sequence. This is only used in\n",
      " |          'labeled' training. If used this must be comprised of n lists where\n",
      " |          n is the number of sequences to train on, and each of those lists\n",
      " |          must have one label per observation. A None in this list corresponds\n",
      " |          to no labels for the entire sequence and triggers semi-supervised\n",
      " |          learning, where the labeled sequences are summarized using labeled\n",
      " |          fitting and the unlabeled are summarized using the specified algorithm.\n",
      " |          Default is None.\n",
      " |      \n",
      " |      algorithm : 'baum-welch', 'viterbi', 'labeled'\n",
      " |          The training algorithm to use. Baum-Welch uses the forward-backward\n",
      " |          algorithm to train using a version of structured EM. Viterbi\n",
      " |          iteratively runs the sequences through the Viterbi algorithm and\n",
      " |          then uses hard assignments of observations to states using that.\n",
      " |          Default is 'baum-welch'. Labeled training requires that labels\n",
      " |          are provided for each observation in each sequence.\n",
      " |      \n",
      " |      inertia : double or None, optional, range [0, 1]\n",
      " |          If double, will set both edge_inertia and distribution_inertia to\n",
      " |          be that value. If None, will not override those values. Default is\n",
      " |          None.\n",
      " |      \n",
      " |      edge_inertia : bool, optional, range [0, 1]\n",
      " |          Whether to use inertia when updating the transition probability\n",
      " |          parameters. Default is 0.0.\n",
      " |      \n",
      " |      distribution_inertia : double, optional, range [0, 1]\n",
      " |          Whether to use inertia when updating the distribution parameters.\n",
      " |          Default is 0.0.\n",
      " |      \n",
      " |      pseudocount : double, optional\n",
      " |          A pseudocount to add to both transitions and emissions. If supplied,\n",
      " |          it will override both transition_pseudocount and emission_pseudocount\n",
      " |          in the same way that specifying `inertia` will override both\n",
      " |          `edge_inertia` and `distribution_inertia`. Default is None.\n",
      " |      \n",
      " |      transition_pseudocount : double, optional\n",
      " |          A pseudocount to add to all transitions to add a prior to the\n",
      " |          MLE estimate of the transition probability. Default is 0.\n",
      " |      \n",
      " |      emission_pseudocount : double, optional\n",
      " |          A pseudocount to add to the emission of each distribution. This\n",
      " |          effectively smoothes the states to prevent 0. probability symbols\n",
      " |          if they don't happen to occur in the data. Only effects hidden\n",
      " |          Markov models defined over discrete distributions. Default is 0.\n",
      " |      \n",
      " |      use_pseudocount : bool, optional\n",
      " |          Whether to use the pseudocounts defined in the `add_edge` method\n",
      " |          for edge-specific pseudocounts when updating the transition\n",
      " |          probability parameters. Does not effect the `transition_pseudocount`\n",
      " |          and `emission_pseudocount` parameters, but can be used in addition\n",
      " |          to them. Default is False.\n",
      " |      \n",
      " |      stop_threshold : double, optional\n",
      " |          The threshold the improvement ratio of the models log probability\n",
      " |          in fitting the scores. Default is 1e-9.\n",
      " |      \n",
      " |      min_iterations : int, optional\n",
      " |          The minimum number of iterations to run Baum-Welch training for.\n",
      " |          Default is 0.\n",
      " |      \n",
      " |      max_iterations : int, optional\n",
      " |          The maximum number of iterations to run Baum-Welch training for.\n",
      " |          Default is 1e8.\n",
      " |      \n",
      " |      n_init : int, optional\n",
      " |          The number of times to initialize the k-means clustering before\n",
      " |          taking the best value. Default is 1.\n",
      " |      \n",
      " |      init : str, optional\n",
      " |          The initialization method for kmeans. Must be one of 'first-k',\n",
      " |          'random', 'kmeans++', or 'kmeans||'. Default is kmeans++.\n",
      " |      \n",
      " |      max_kmeans_iterations : int, optional\n",
      " |          The number of iterations to run k-means for before starting EM.\n",
      " |      \n",
      " |      batch_size : int or None, optional\n",
      " |          The number of samples in a batch to summarize on. This controls\n",
      " |          the size of the set sent to `summarize` and so does not make the\n",
      " |          update any less exact. This is useful when training on a memory\n",
      " |          map and cannot load all the data into memory. If set to None,\n",
      " |          batch_size is 1 / n_jobs. Default is None.\n",
      " |      \n",
      " |      batches_per_epoch : int or None, optional\n",
      " |          The number of batches in an epoch. This is the number of batches to\n",
      " |          summarize before calling `from_summaries` and updating the model\n",
      " |          parameters. This allows one to do minibatch updates by updating the\n",
      " |          model parameters before setting the full dataset. If set to None,\n",
      " |          uses the full dataset. Default is None.\n",
      " |      \n",
      " |      lr_decay : double, optional, positive\n",
      " |          The step size decay as a function of the number of iterations.\n",
      " |          Functionally, this sets the inertia to be (2+k)^{-lr_decay}\n",
      " |          where k is the number of iterations. This causes initial\n",
      " |          iterations to have more of an impact than later iterations,\n",
      " |          and is frequently used in minibatch learning. This value is\n",
      " |          suggested to be between 0.5 and 1. Default is 0, meaning no\n",
      " |          decay.\n",
      " |      \n",
      " |      end_state : bool, optional\n",
      " |          Whether to calculate the probability of ending in each state or not.\n",
      " |          Default is False.\n",
      " |      \n",
      " |      state_names : array-like, shape (n_states), optional\n",
      " |          The name of the states. If None is passed in, default names are\n",
      " |          generated. Default is None\n",
      " |      \n",
      " |      name : str, optional\n",
      " |          The name of the model. Default is None\n",
      " |      \n",
      " |      callbacks : list, optional\n",
      " |          A list of callback objects that describe functionality that should\n",
      " |          be undertaken over the course of training.\n",
      " |      \n",
      " |      return_history : bool, optional\n",
      " |          Whether to return the history during training as well as the model.\n",
      " |      \n",
      " |      verbose : bool, optional\n",
      " |          Whether to print the improvement in the model fitting at each\n",
      " |          iteration. Default is True.\n",
      " |      \n",
      " |      n_jobs : int, optional\n",
      " |          The number of threads to use when performing training. This\n",
      " |          leads to exact updates. Default is 1.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      model : HiddenMarkovModel\n",
      " |          The model fit to the data.\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Static methods defined here:\n",
      " |  \n",
      " |  __new__(*args, **kwargs) from builtins.type\n",
      " |      Create and return a new object.  See help(type) for accurate signature.\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Data descriptors defined here:\n",
      " |  \n",
      " |  discrete\n",
      " |  \n",
      " |  end\n",
      " |  \n",
      " |  end_index\n",
      " |  \n",
      " |  keymap\n",
      " |  \n",
      " |  multivariate\n",
      " |  \n",
      " |  silent_start\n",
      " |  \n",
      " |  start\n",
      " |  \n",
      " |  start_index\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Data and other attributes defined here:\n",
      " |  \n",
      " |  __pyx_vtable__ = <capsule object NULL>\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Methods inherited from pomegranate.base.GraphModel:\n",
      " |  \n",
      " |  __str__(...)\n",
      " |      Represent this model with it's name and states.\n",
      " |  \n",
      " |  add_edge(...)\n",
      " |      Add a transition from state a to state b which indicates that B is\n",
      " |      dependent on A in ways specified by the distribution.\n",
      " |  \n",
      " |  add_node(...)\n",
      " |      Add a node to the graph.\n",
      " |  \n",
      " |  add_nodes(...)\n",
      " |      Add multiple states to the graph.\n",
      " |  \n",
      " |  edge_count(...)\n",
      " |      Returns the number of edges present in the model.\n",
      " |  \n",
      " |  node_count(...)\n",
      " |      Returns the number of nodes/states in the model\n",
      " |  \n",
      " |  state_count(...)\n",
      " |      Returns the number of states present in the model.\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Data descriptors inherited from pomegranate.base.GraphModel:\n",
      " |  \n",
      " |  edges\n",
      " |  \n",
      " |  graph\n",
      " |  \n",
      " |  states\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Methods inherited from pomegranate.base.Model:\n",
      " |  \n",
      " |  __repr__(self, /)\n",
      " |      Return repr(self).\n",
      " |  \n",
      " |  freeze(...)\n",
      " |      Freeze the distribution, preventing updates from occurring.\n",
      " |  \n",
      " |  get_params(...)\n",
      " |  \n",
      " |  probability(...)\n",
      " |      Return the probability of the given symbol under this distribution.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      symbol : object\n",
      " |              The symbol to calculate the probability of\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      probability : double\n",
      " |              The probability of that point under the distribution.\n",
      " |  \n",
      " |  score(...)\n",
      " |      Return the accuracy of the model on a data set.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      X : numpy.ndarray, shape=(n, d)\n",
      " |              The values of the data set\n",
      " |      \n",
      " |      y : numpy.ndarray, shape=(n,)\n",
      " |              The labels of each value\n",
      " |  \n",
      " |  set_params(...)\n",
      " |  \n",
      " |  thaw(...)\n",
      " |      Thaw the distribution, re-allowing updates to occur.\n",
      " |  \n",
      " |  to_yaml(...)\n",
      " |      Serialize the model to YAML for compactness.\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Class methods inherited from pomegranate.base.Model:\n",
      " |  \n",
      " |  from_yaml(...) from builtins.type\n",
      " |      Deserialize this object from its YAML representation.\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Data descriptors inherited from pomegranate.base.Model:\n",
      " |  \n",
      " |  d\n",
      " |  \n",
      " |  frozen\n",
      " |  \n",
      " |  model\n",
      " |  \n",
      " |  name\n",
      "\n"
     ]
    }
   ],
   "source": [
    "help(basic_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dict_probs(emission_count,tag):\n",
    "    if tag==\"\":\n",
    "        return dict(zip(list(emission_count.keys()),np.array(list(emission_count.values()))/sum(emission_count.values())))\n",
    "    else:\n",
    "        return dict(zip(list(emission_count[tag].keys()),np.array(list(emission_count[tag].values()))/sum(emission_count[tag].values())))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div class=\"alert alert-block alert-success\">Your HMM network topology looks good!</div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "execution_count": 152,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "basic_model = HiddenMarkovModel(name=\"base-hmm-tagger\")\n",
    "\n",
    "# TODO: create states with emission probability distributions P(word | tag) and add to the model\n",
    "# (Hint: you may need to loop & create/add new states)\n",
    "tag_sum = sum(tag_unigrams.values())\n",
    "tag_stat_dict = dict()\n",
    "\n",
    "for tag in data.tagset:\n",
    "    stat = dict_probs(emission_counts,tag)\n",
    "    d = DiscreteDistribution(stat)\n",
    "    fr= State(d,name=tag)\n",
    "    basic_model.add_state(fr)\n",
    "    tag_stat_dict[tag]=fr\n",
    "start_probs = dict_probs(tag_starts,\"\")\n",
    "for st_tag in data.tagset:\n",
    "    basic_model.add_transition(basic_model.start,tag_stat_dict[st_tag],start_probs[st_tag])\n",
    "for tag1 in data.tagset:\n",
    "    probs = tag_ends[tag1]\n",
    "    for tag2 in data.tagset:\n",
    "        probs+=tag_bigrams[(tag1,tag2)]\n",
    "\n",
    "    for tag2 in data.tagset:\n",
    "        basic_model.add_transition(tag_stat_dict[tag1],tag_stat_dict[tag2],tag_bigrams[(tag1,tag2)]/probs+1e-6)\n",
    "    basic_model.add_transition(tag_stat_dict[tag1],basic_model.end,tag_ends[tag1]/probs)\n",
    "\n",
    "\n",
    "# TODO: add edges between states for the observed transition frequencies P(tag_i | tag_i-1)\n",
    "# (Hint: you may need to loop & add transitions\n",
    "    \n",
    "\n",
    "\n",
    "# NOTE: YOU SHOULD NOT NEED TO MODIFY ANYTHING BELOW THIS LINE\n",
    "# finalize the model\n",
    "basic_model.bake()\n",
    "\n",
    "assert all(tag in set(s.name for s in basic_model.states) for tag in data.training_set.tagset), \\\n",
    "       \"Every state in your network should use the name of the associated tag, which must be one of the training set tags.\"\n",
    "assert basic_model.edge_count() == 168, \\\n",
    "       (\"Your network should have an edge from the start node to each state, one edge between every \" +\n",
    "        \"pair of tags (states), and an edge from each state to the end node.\")\n",
    "HTML('<div class=\"alert alert-block alert-success\">Your HMM network topology looks good!</div>')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training accuracy basic hmm model: 97.24%\n",
      "testing accuracy basic hmm model: 95.67%\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div class=\"alert alert-block alert-success\">Your HMM tagger accuracy looks correct! Congratulations, you've finished the project.</div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "execution_count": 153,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hmm_training_acc = accuracy(data.training_set.X, data.training_set.Y, basic_model)\n",
    "print(\"training accuracy basic hmm model: {:.2f}%\".format(100 * hmm_training_acc))\n",
    "\n",
    "hmm_testing_acc = accuracy(data.testing_set.X, data.testing_set.Y, basic_model)\n",
    "print(\"testing accuracy basic hmm model: {:.2f}%\".format(100 * hmm_testing_acc))\n",
    "\n",
    "assert hmm_training_acc > 0.97, \"Uh oh. Your HMM accuracy on the training set doesn't look right.\"\n",
    "assert hmm_testing_acc > 0.955, \"Uh oh. Your HMM accuracy on the testing set doesn't look right.\"\n",
    "HTML('<div class=\"alert alert-block alert-success\">Your HMM tagger accuracy looks correct! Congratulations, you\\'ve finished the project.</div>')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example Decoding Sequences with the HMM Tagger"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sentence Key: b100-28144\n",
      "\n",
      "Predicted labels:\n",
      "-----------------\n",
      "['CONJ', 'NOUN', 'NUM', '.', 'NOUN', 'NUM', '.', 'NOUN', 'NUM', '.', 'CONJ', 'NOUN', 'NUM', '.', '.', 'NOUN', '.', '.']\n",
      "\n",
      "Actual labels:\n",
      "--------------\n",
      "('CONJ', 'NOUN', 'NUM', '.', 'NOUN', 'NUM', '.', 'NOUN', 'NUM', '.', 'CONJ', 'NOUN', 'NUM', '.', '.', 'NOUN', '.', '.')\n",
      "\n",
      "\n",
      "Sentence Key: b100-23146\n",
      "\n",
      "Predicted labels:\n",
      "-----------------\n",
      "['PRON', 'VERB', 'DET', 'NOUN', 'ADP', 'ADJ', 'ADJ', 'NOUN', 'VERB', 'VERB', '.', 'ADP', 'VERB', 'DET', 'NOUN', 'ADP', 'NOUN', 'ADP', 'DET', 'NOUN', '.']\n",
      "\n",
      "Actual labels:\n",
      "--------------\n",
      "('PRON', 'VERB', 'DET', 'NOUN', 'ADP', 'ADJ', 'ADJ', 'NOUN', 'VERB', 'VERB', '.', 'ADP', 'VERB', 'DET', 'NOUN', 'ADP', 'NOUN', 'ADP', 'DET', 'NOUN', '.')\n",
      "\n",
      "\n",
      "Sentence Key: b100-35462\n",
      "\n",
      "Predicted labels:\n",
      "-----------------\n",
      "['DET', 'ADJ', 'NOUN', 'VERB', 'VERB', 'VERB', 'ADP', 'DET', 'ADJ', 'ADJ', 'NOUN', 'ADP', 'DET', 'ADJ', 'NOUN', '.', 'ADP', 'ADJ', 'NOUN', '.', 'CONJ', 'ADP', 'DET', 'NOUN', 'ADP', 'ADJ', 'ADJ', '.', 'ADJ', '.', 'CONJ', 'ADJ', 'NOUN', 'ADP', 'ADJ', 'NOUN', '.']\n",
      "\n",
      "Actual labels:\n",
      "--------------\n",
      "('DET', 'ADJ', 'NOUN', 'VERB', 'VERB', 'VERB', 'ADP', 'DET', 'ADJ', 'ADJ', 'NOUN', 'ADP', 'DET', 'ADJ', 'NOUN', '.', 'ADP', 'ADJ', 'NOUN', '.', 'CONJ', 'ADP', 'DET', 'NOUN', 'ADP', 'ADJ', 'ADJ', '.', 'ADJ', '.', 'CONJ', 'ADJ', 'NOUN', 'ADP', 'ADJ', 'NOUN', '.')\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for key in data.testing_set.keys[:3]:\n",
    "    print(\"Sentence Key: {}\\n\".format(key))\n",
    "    print(\"Predicted labels:\\n-----------------\")\n",
    "    print(simplify_decoding(data.sentences[key].words, basic_model))\n",
    "    print()\n",
    "    print(\"Actual labels:\\n--------------\")\n",
    "    print(data.sentences[key].tags)\n",
    "    print(\"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## Finishing the project\n",
    "---\n",
    "\n",
    "<div class=\"alert alert-block alert-info\">\n",
    "**Note:** **SAVE YOUR NOTEBOOK**, then run the next cell to generate an HTML copy. You will zip & submit both this file and the HTML copy for review.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['[NbConvertApp] Converting notebook HMM Tagger.ipynb to html',\n",
       " '[NbConvertApp] Writing 441016 bytes to HMM Tagger.html',\n",
       " '[NbConvertApp] Converting notebook HMM warmup (optional).ipynb to html',\n",
       " '[NbConvertApp] Writing 334059 bytes to HMM warmup (optional).html']"
      ]
     },
     "execution_count": 147,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "!!jupyter nbconvert *.ipynb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 4: [Optional] Improving model performance\n",
    "---\n",
    "There are additional enhancements that can be incorporated into your tagger that improve performance on larger tagsets where the data sparsity problem is more significant. The data sparsity problem arises because the same amount of data split over more tags means there will be fewer samples in each tag, and there will be more missing data  tags that have zero occurrences in the data. The techniques in this section are optional.\n",
    "\n",
    "- [Laplace Smoothing](https://en.wikipedia.org/wiki/Additive_smoothing) (pseudocounts)\n",
    "    Laplace smoothing is a technique where you add a small, non-zero value to all observed counts to offset for unobserved values.\n",
    "\n",
    "- Backoff Smoothing\n",
    "    Another smoothing technique is to interpolate between n-grams for missing data. This method is more effective than Laplace smoothing at combatting the data sparsity problem. Refer to chapters 4, 9, and 10 of the [Speech & Language Processing](https://web.stanford.edu/~jurafsky/slp3/) book for more information.\n",
    "\n",
    "- Extending to Trigrams\n",
    "    HMM taggers have achieved better than 96% accuracy on this dataset with the full Penn treebank tagset using an architecture described in [this](http://www.coli.uni-saarland.de/~thorsten/publications/Brants-ANLP00.pdf) paper. Altering your HMM to achieve the same performance would require implementing deleted interpolation (described in the paper), incorporating trigram probabilities in your frequency tables, and re-implementing the Viterbi algorithm to consider three consecutive states instead of two.\n",
    "\n",
    "### Obtain the Brown Corpus with a Larger Tagset\n",
    "Run the code below to download a copy of the brown corpus with the full NLTK tagset. You will need to research the available tagset information in the NLTK docs and determine the best way to extract the subset of NLTK tags you want to explore. If you write the following the format specified in Step 1, then you can reload the data using all of the code above for comparison.\n",
    "\n",
    "Refer to [Chapter 5](http://www.nltk.org/book/ch05.html) of the NLTK book for more information on the available tagsets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Error loading brown: <urlopen error [Errno 11001]\n",
      "[nltk_data]     getaddrinfo failed>\n"
     ]
    },
    {
     "ename": "LookupError",
     "evalue": "\n**********************************************************************\n  Resource \u001b[93mbrown\u001b[0m not found.\n  Please use the NLTK Downloader to obtain the resource:\n\n  \u001b[31m>>> import nltk\n  >>> nltk.download('brown')\n  \u001b[0m\n  Searched in:\n    - 'C:\\\\Users\\\\ADMIN/nltk_data'\n    - 'C:\\\\nltk_data'\n    - 'D:\\\\nltk_data'\n    - 'E:\\\\nltk_data'\n    - 'C:\\\\Users\\\\ADMIN\\\\Anaconda3\\\\nltk_data'\n    - 'C:\\\\Users\\\\ADMIN\\\\Anaconda3\\\\share\\\\nltk_data'\n    - 'C:\\\\Users\\\\ADMIN\\\\Anaconda3\\\\lib\\\\nltk_data'\n    - 'C:\\\\Users\\\\ADMIN\\\\AppData\\\\Roaming\\\\nltk_data'\n**********************************************************************\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mLookupError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\nltk\\corpus\\util.py\u001b[0m in \u001b[0;36m__load\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m     79\u001b[0m             \u001b[1;32mexcept\u001b[0m \u001b[0mLookupError\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 80\u001b[1;33m                 \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mroot\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnltk\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfind\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'{}/{}'\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msubdir\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mzip_name\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     81\u001b[0m                 \u001b[1;32mexcept\u001b[0m \u001b[0mLookupError\u001b[0m\u001b[1;33m:\u001b[0m \u001b[1;32mraise\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\nltk\\data.py\u001b[0m in \u001b[0;36mfind\u001b[1;34m(resource_name, paths)\u001b[0m\n\u001b[0;32m    674\u001b[0m     \u001b[0mresource_not_found\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m'\\n%s\\n%s\\n%s\\n'\u001b[0m \u001b[1;33m%\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0msep\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmsg\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msep\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 675\u001b[1;33m     \u001b[1;32mraise\u001b[0m \u001b[0mLookupError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mresource_not_found\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    676\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mLookupError\u001b[0m: \n**********************************************************************\n  Resource \u001b[93mbrown\u001b[0m not found.\n  Please use the NLTK Downloader to obtain the resource:\n\n  \u001b[31m>>> import nltk\n  >>> nltk.download('brown')\n  \u001b[0m\n  Searched in:\n    - 'C:\\\\Users\\\\ADMIN/nltk_data'\n    - 'C:\\\\nltk_data'\n    - 'D:\\\\nltk_data'\n    - 'E:\\\\nltk_data'\n    - 'C:\\\\Users\\\\ADMIN\\\\Anaconda3\\\\nltk_data'\n    - 'C:\\\\Users\\\\ADMIN\\\\Anaconda3\\\\share\\\\nltk_data'\n    - 'C:\\\\Users\\\\ADMIN\\\\Anaconda3\\\\lib\\\\nltk_data'\n    - 'C:\\\\Users\\\\ADMIN\\\\AppData\\\\Roaming\\\\nltk_data'\n**********************************************************************\n",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[1;31mLookupError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-148-e64fc0cb7ba3>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[0mnltk\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdownload\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'brown'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[0mtraining_corpus\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnltk\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcorpus\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbrown\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 7\u001b[1;33m \u001b[0mtraining_corpus\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtagged_sents\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\nltk\\corpus\\util.py\u001b[0m in \u001b[0;36m__getattr__\u001b[1;34m(self, attr)\u001b[0m\n\u001b[0;32m    114\u001b[0m             \u001b[1;32mraise\u001b[0m \u001b[0mAttributeError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"LazyCorpusLoader object has no attribute '__bases__'\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    115\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 116\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__load\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    117\u001b[0m         \u001b[1;31m# This looks circular, but its not, since __load() changes our\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    118\u001b[0m         \u001b[1;31m# __class__ to something new:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\nltk\\corpus\\util.py\u001b[0m in \u001b[0;36m__load\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m     79\u001b[0m             \u001b[1;32mexcept\u001b[0m \u001b[0mLookupError\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     80\u001b[0m                 \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mroot\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnltk\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfind\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'{}/{}'\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msubdir\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mzip_name\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 81\u001b[1;33m                 \u001b[1;32mexcept\u001b[0m \u001b[0mLookupError\u001b[0m\u001b[1;33m:\u001b[0m \u001b[1;32mraise\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     82\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     83\u001b[0m         \u001b[1;31m# Load the corpus.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\nltk\\corpus\\util.py\u001b[0m in \u001b[0;36m__load\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m     76\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     77\u001b[0m             \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 78\u001b[1;33m                 \u001b[0mroot\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnltk\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfind\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'{}/{}'\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msubdir\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__name\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     79\u001b[0m             \u001b[1;32mexcept\u001b[0m \u001b[0mLookupError\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     80\u001b[0m                 \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mroot\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnltk\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfind\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'{}/{}'\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msubdir\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mzip_name\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\nltk\\data.py\u001b[0m in \u001b[0;36mfind\u001b[1;34m(resource_name, paths)\u001b[0m\n\u001b[0;32m    673\u001b[0m     \u001b[0msep\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m'*'\u001b[0m \u001b[1;33m*\u001b[0m \u001b[1;36m70\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    674\u001b[0m     \u001b[0mresource_not_found\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m'\\n%s\\n%s\\n%s\\n'\u001b[0m \u001b[1;33m%\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0msep\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmsg\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msep\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 675\u001b[1;33m     \u001b[1;32mraise\u001b[0m \u001b[0mLookupError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mresource_not_found\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    676\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    677\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mLookupError\u001b[0m: \n**********************************************************************\n  Resource \u001b[93mbrown\u001b[0m not found.\n  Please use the NLTK Downloader to obtain the resource:\n\n  \u001b[31m>>> import nltk\n  >>> nltk.download('brown')\n  \u001b[0m\n  Searched in:\n    - 'C:\\\\Users\\\\ADMIN/nltk_data'\n    - 'C:\\\\nltk_data'\n    - 'D:\\\\nltk_data'\n    - 'E:\\\\nltk_data'\n    - 'C:\\\\Users\\\\ADMIN\\\\Anaconda3\\\\nltk_data'\n    - 'C:\\\\Users\\\\ADMIN\\\\Anaconda3\\\\share\\\\nltk_data'\n    - 'C:\\\\Users\\\\ADMIN\\\\Anaconda3\\\\lib\\\\nltk_data'\n    - 'C:\\\\Users\\\\ADMIN\\\\AppData\\\\Roaming\\\\nltk_data'\n**********************************************************************\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk import pos_tag, word_tokenize\n",
    "from nltk.corpus import brown\n",
    "\n",
    "nltk.download('brown')\n",
    "training_corpus = nltk.corpus.brown\n",
    "training_corpus.tagged_sents()[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
